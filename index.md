## Perspectives on Applied Statistics

Assembled by C. Tong.  This is a living, growing page, so not the last word.

### John Kay and Mervyn King

"Resolvable uncertainty is uncertainty which can be removed by looking something up (I am uncertain which city is the capital of Pennsylvania) or which can be represented by a known probability distribution of outcomes (the spin of a roulette wheel).  With radical uncertainty, however, there is no similar means of resolving the uncertainty -- we simply do not know.  Radical uncertainty has many dimensions:  obscurity; ignorance; vagueness; ambiguity; ill-defined problems; and a lack of information that in some cases but not all we might hope to rectify at a future date.  Those aspects of uncertainty are the stuff of everyday experience.

"Radical uncertainty cannot be described in the probabilistic terms applicable to a game of chance.  It is not just that we do not know what will happen.  We often do not even know the kinds of things that might happen.  When we describe radical uncertainty we are not talking about 'long tails' -- imaginable and well-defined events whose probability can be estimated, such as a long losing streak at roulette.  And we are not only talking about the 'black swans' identified by Nassim Nicholas Taleb -- surprising events which no one could have anticipated until they happen, although these 'black swans' *are* examples of radical uncertainty.  We are emphasizing the vast range of possibilities that lie in between the world of unlikely events which can nevertheless be described with the aid of probability distributions, and the world of the unimaginable.  This is a world of uncertain futures and unpredictable consequences, about which there is necessary speculation and inevitable disagreement -- disagreement which often will never be resolved.  And it is that world which we mostly encounter."

 Source:  Excerpt from John Kay and Mervyn King (2020), *Radical Uncertainty:  Decision-Making Beyond the Numbers* (New York:  W. W. Norton), Ch. 1 ("The Unknowable Future").

### John W. Tukey

"Let us return, then, to the original question:  What ought to be the nature of data analysis?

"Data analysis needs to be both exploratory and confirmatory. In exploratory data analysis there can be no substitute for flexibility, for adapting what is calculated -- and, we hope, plotted -- both to the needs of the situation and the clues that the data have already provided. In this mode, data analysis is detective work -- almost an ideal example of seeking what might be relevant.

"Confirmatory data analysis has its place, too.  Well used, its importance may even equal that of exploratory data analysis. We dare not, however, let it be an imprimatur or a testimony of infallibility.  'Not a high priestess but a handmaiden' must be our demand. Confirmatory data analysis must be the means by which we adjust optimism and pessimism, not only ours but those of our readers. To do this is not easy and may require new approaches and unfamiliar ways of thinking.  \[....\]

"Data analysis has its major uses. They are detective work and guidance counseling. Let us all try to act accordingly."

Source:  Exceprt from the final ("Detective Work versus Sanctification") section of John W. Tukey (1969), "Data Analysis:  Sanctification or Detective Work?", *American Psychologist*, 24 (2):  83-91.

### David A. Freedman

"Statisticians generally prefer to make causal inferences from randomized controlled experiments, using the techniques developed by \[Ronald A.\] Fisher and \[Jerzy\] Neyman. In many situations, of course, experiments are impractical or unethical. Most of what we know about causation in such contexts is derived from observational studies. Sometimes, these are analyzed by regression models; sometimes, these are treated as natural experiments, perhaps after conditioning on covariates. Delicate judgments are required to assess the probable impact of confounders (measured and unmeasured), other sources of bias, and the adequacy of the statistical models used to make adjustments. There is much room for error in this enterprise, and much room for legitimate disagreement.

"\[John\] Snow’s work on cholera, among other examples, shows that sound causal inferences can be drawn from nonexperimental data. On the one hand, no mechanical rules can be laid down for making such inferences. Since \[David\] Hume’s day, that is almost a truism. On the other hand, an enormous investment of skill, intelligence and hard work seems to be a requirement. Many convergent lines of evidence must be developed. Natural variation needs to be identified and exploited. Data must be collected. Confounders need to be considered. Alternative explanations have to be exhaustively tested. Above all, the right question needs to be framed.

"Naturally, there is a strong desire to substitute intellectual capital for labor. That is why investigators often try to base causal inference on statistical models. With this approach, *P*-values play a crucial role. The technology is relatively easy to use and promises to open a wide variety of questions to the research effort. However, the appearance of methodological rigor can be deceptive. Like confidence intervals, *P*-values generally deal with the problem of sampling error not the problem of bias. Even with sampling error, artifactual results are likely if there is any kind of search over possible specifications for a model, or different definitions of exposure and disease. Models may be used in efforts to adjust for confounding and other sources of bias, but many somewhat arbitrary choices are made. Which variables to enter in the equation? What functional form to use? What assumptions to make about error terms?  These choices are seldom dictated either by data or prior scientific knowledge. That is why judgment is so critical, the opportunity for error so large and the number of successful applications so limited."

Source:  Excerpt from the final ("Summary and Conclusions") section of David Freedman (1999), "From Association to Causation: Some Remarks on the History of Statistics", *Statistical Science*, 14 (3):  243-258.

### George E. P. Box

"Statistics has no reason for existence except as a catalyst for scientific enquiry in which only the last stage, when all the creative work has already been done, is concerned with a final fixed model and a rigorous test of conclusions. The main part of such an investigation involves an inductive-deductive iteration with input coming from the subject-matter specialist at every stage. This requires a continuously developing model in which the identity of the measured responses, the factors considered, the structure of the mathematical model, the number and nature of its parameters and even the objective
of the study change. With its present access to enormous computer power and provocative and thought-provoking graphical display, modern statistics could make enormous contributions to this -- the main body of scientific endeavour. But most of the time it does not." 

Source:  Excerpt from G. E. P. Box's *Discussion* of David Draper (1995), "Assessment and Propagation of Model Uncertainty" (with discussion), *Journal of the Royal Statistical Society*, Series B, 57 (1):  45–97.


## Required reading for all applied statisticians and data professionals (IMHO)

D. G. Altman and J. M. Bland, 1995: Absence of evidence is not evidence of absence. *British Medical Journal*, 311: 485.

V. Amrhein, D. Trafimow, and S. Greenland, 2019:  Inferential statistics as descriptive statistics:  there is no replication crisis if we don’t expect replication.  *The American Statistician*, 73 (sup1):  262-270.

G. E. P. Box, 1976:  Science and statistics.  *Journal of the American Statistical Association*, 71:  791-799.

G. E. P. Box, 2001:  Statistics for discovery.  *Journal of Applied Statistics*, 28:  285-299.

L. Breiman, 2001:  Statistical modeling:  The two cultures (with discussion).  *Statistical Science*, 16:  199-231.

C. Chatfield, 1995:  Model uncertainty, data mining and statistical inference (with discussion).  *Journal of the Royal Statistical Society*, Series A, 158:  419-466.

W. E. Deming, 1975:  On probability as a basis for action.  *The American Statistician*, 29:  146-152.

P. Diaconis, 1985:  Theories of data analysis:  From magical thinking through classical statistics.  In *Exploring Data Tables, Trends, and Shapes*, ed. by D. C. Hoaglin, F. Mosteller, and J. W. Tukey (New York:  Wiley), 1-36.

D. Donoho, 2017:  50 years of data science (with discussion).  *Journal of Computational and Graphical Statistics*, 26:  745-785.

A. S. C. Ehrenberg, 1990:  A hope for the future of statistics:  MSOD.  *The American Statistician*, 44:  195-196.

A. S. C. Ehrenberg and J. A. Bound, 1993:  Predictability and prediction.  *Journal of the Royal Statistical Society*, Series A, 156:  167-206.

D. A. Freedman, 1999:  From association to causation:  some remarks on the history of statistics.  *Statistical Science*, 14:  243-258.

J. H. Friedman, 2001: The role of statistics in the data revolution? *International Statistical Review*, 69: 5-10.

A. Gelman and E. Loken, 2014:  The statistical crisis in science.  *American Scientist*, 102:  460-465.

G. Gigerenzer and J. Marewski, 2015:  Surrogate science:  The idol of a universal method for scientific inference.  *Journal of Management*, 41:  421-440.

S. Greenland, 2017:  For and against methodologies:  some perspectives on recent causal and statistical inference debates.  *European Journal of Epidemiology*, 32:  3-20.

S. Greenland, 2017:  The need for cognitive science in methodology.  *American Journal of Epidemiology*, 186:  639-645.

B. Hayes, 2001: Randomness as a resource. *American Scientist*, 89: 300-304.

J. R. Hollenbeck and P. M. Wright, 2016:  Harking, sharking, and tharking:  Making the case for post hoc analysis of scientific data.  *Journal of Management*, 43:  5-18.

M. Lavine, 2019:  Frequentist, Bayes, or other?  *The American Statistican*, 73 (sup1):  312-318.

G. J. Lithgow, M. Driscoll, and P. Phillips, 2017:  A long journey to reproducible results.  *Nature*, 548:  387-388.

J. J. Locascio, 2019:  The impact of results blind scientific publishing on statistical consulation and collaboration.  *The American Statistician*, 73 (sup1):  346-351.

B. B. McShane, D. Gal, A. Gelman, C. Robert, and J. L. Tackett, 2019:  Abandon statistical significance.  *The American Statistician*, 73 (sup1):  235-245.

C. Mallows, 1998:  The zeroth problem.  *The American Statistician*, 52:  1-9.

J. S. Mogil and M. R. Macleod, 2017:  No publication without confirmation.  *Nature*, 542:  409-411.

M. R. Munafo and G. Davey Smith, 2018:  Robust research needs many lines of evidence.  *Nature*, 553:  399-401.

H. Quinn, 2009:  What is science?  *Physics Today*, 62 (7):  8-9.

M. Schrage, 2000: How the bell curve cheats you. *Fortune*, 21 February 2000, p. 296.

C. Seife, 2000:  CERN’s gamble shows perils, rewards of playing the odds.  *Science*, 289:  2260-2262.

L. Shepp, 2007:  Statistical thinking:  From Tukey to Vardi and beyond.   In *Complex Datasets and Inverse Problems:  Tomography, Networks and Beyond*, ed. by R. Liu, W. Strawderman, and C.-H. Zhang.    IMS Lecture Notes-Monograph Series, 54:  268-273.

J. P. Simmons, L. D. Nelson, and U. Simonsohn, 2011:  False-positive psychology:  undisclosed flexibility in data collection and analysis allows presenting anything as significant.  *Psychological Science*, 22:  1359-1366.

J. W. Tukey, 1980:  We need both exploratory and confirmatory.  *American Statistician*, 34:  23-25.

